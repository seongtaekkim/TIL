7번 OPA

8번

![스크린샷 2023-08-17 오전 1.58.05](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-17 오전 1.58.05.png)

1. Allow users to "skip login"
2. Allow insecure access (HTTP without authentication)
3. Allow basic authentication
4. Allow access from outside the cluster

1. Deny users to "skip login"
2. Deny insecure access, enforce HTTPS (self signed certificates are ok for now)
3. Add the `--auto-generate-certificates` argument
4. Enforce authentication using a token (with possibility to use RBAC)

![스크린샷 2023-08-17 오전 2.05.37](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-17 오전 2.05.37.png)

![스크린샷 2023-08-17 오전 2.05.16](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-17 오전 2.05.16.png)

## ![스크린샷 2023-08-17 오전 2.08.18](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-17 오전 2.08.18.png)



13번 

![스크린샷 2023-08-17 오전 2.27.00](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-17 오전 2.27.00.png)

1번케이스가 모두 막고

2번케이스가 pod3만 열어논흔ㄴ다.

17 audit 흠..

![스크린샷 2023-08-17 오전 2.46.21](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-17 오전 2.46.21.png)

![스크린샷 2023-08-17 오전 2.46.32](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-17 오전 2.46.32.png)





시크릿 사용 방법

![스크린샷 2023-08-17 오전 2.59.33](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-17 오전 2.59.33.png)

## ![스크린샷 2023-08-17 오전 3.00.00](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-17 오전 3.00.00.png)



https://www.certshero.com/linux-foundation/cks/practice-test



https://www.certshero.com/linux-foundation/cks/practice-test



https://killer.sh/attendee/ba2db82a-1054-4743-b0f7-f49064d7d167/content

https://velog.io/@comeonyo/CKS-%EC%8B%9C%ED%97%98-%EB%B2%94%EC%9C%84-%EC%A0%95%EB%A6%AC

controlplane ~ ✖ k create configmap untrusted-registry --from-file=untrusted-registry.rego



~~~
FROM alpine:3.12.1
COPY --from=0 /app .
CMD ["./app"]
~~~



automountServiceAccountToken: false

## 0. summary

~~~
securityContext:
  readOnlyRootFilesystem: true
  privileged: false
  runAsUser: 1000
  runAsGroup: 3000
  fsGroup: 2000
  fsGroupChangePolicy: "OnRootMismatch"
  allowPrivilegeEscalation: false
  capabilities:
    add: ["NET_ADMIN", "SYS_TIME"]
~~~

~~~
aa-status
apparmor_parser
/etc/apparmor.d/
annotations:
  container.apparmor.security.beta.kubernetes.io/<container_name>: <profile_ref>
~~~

### falco

~~~
ps aux | grep falco
pidof falco
kill -1 $(pidof falco)
[timestamp],[uid],[processName] ->  %evt.time,%user.uid,%proc.name

Use detection tools to detect anomalies like processes spawning and executing something weird frequently in the single container belonging to Pod tomcat.
-> Container Drift Detected # 이게 뭔소리 ...
 
[timestamp],[uid],[user-name],[processName] %user.name

[timestamp],[uid],[user-name],[processName]
: "%evt.time,%user.uid,%user.name,%proc.name"
~~~

### kube-bench

~~~
kube-bench run --targets=master | grep 요구사항
~~~





### serviceaccount

~~~sh
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-robot
automountServiceAccountToken: false
~~~



### imagepolicywebhook

~~~
$ vim /etc/kubernetes/manifests/kube-apiserver.yaml
- --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook # Add
- --admission-control-config-file=/etc/kubernetes/config/kubeconfig.json # Add
~~~



### csr

~~~

~~~



https://www.certshero.com/linux-foundation/cks/practice-test

### audit

~~~
 --audit-log-maxage defined the maximum number of days to retain old audit log files
 --audit-log-maxbackup defines the maximum number of audit log files to retain
 --audit-log-maxsize defines the maximum size in megabytes of the audit log file

--audit-policy-file=/etc/kubernetes/audit-policy.yaml
--audit-log-path=/var/log/audit.log
~~~

~~~yaml
apiVersion: audit.k8s.io/v1 # This is required.
kind: Policy
# Don't generate audit events for all requests in RequestReceived stage.
omitStages:
  - "RequestReceived"
rules:
  # Log pod changes at RequestResponse level
  - level: RequestResponse
    resources:
    - group: ""
      # Resource "pods" doesn't match requests to any subresource of pods,
      # which is consistent with the RBAC policy.
      resources: ["pods"]
  # Log "pods/log", "pods/status" at Metadata level
  - level: Metadata
    resources:
    - group: ""
      resources: ["pods/log", "pods/status"]

  # Don't log requests to a configmap called "controller-leader"
  - level: None
    resources:
    - group: ""
      resources: ["configmaps"]
      resourceNames: ["controller-leader"]

  # Don't log watch requests by the "system:kube-proxy" on endpoints or services
  - level: None
    users: ["system:kube-proxy"]
    verbs: ["watch"]
    resources:
    - group: "" # core API group
      resources: ["endpoints", "services"]

  # Don't log authenticated requests to certain non-resource URL paths.
  - level: None
    userGroups: ["system:authenticated"]
    nonResourceURLs:
    - "/api*" # Wildcard matching.
    - "/version"

  # Log the request body of configmap changes in kube-system.
  - level: Request
    resources:
    - group: "" # core API group
      resources: ["configmaps"]
    # This rule only applies to resources in the "kube-system" namespace.
    # The empty string "" can be used to select non-namespaced resources.
    namespaces: ["kube-system"]

  # Log configmap and secret changes in all other namespaces at the Metadata level.
  - level: Metadata
    resources:
    - group: "" # core API group
      resources: ["secrets", "configmaps"]

  # Log all other resources in core and extensions at the Request level.
  - level: Request
    resources:
    - group: "" # core API group
    - group: "extensions" # Version of group should NOT be included.

  # A catch-all rule to log all other requests at the Metadata level.
  - level: Metadata
    # Long-running requests like watches that fall under this rule will not
    # generate an audit event in RequestReceived.
    omitStages:
      - "RequestReceived"

~~~

~~~
 spec:
  containers:
      - command:
        - kube-apiserver
        - --audit-policy-file=/etc/kubernetes/audit-policy.yaml
        - --audit-log-path=/var/log/kubernetes/audit/audit.log
        - --audit-log-maxage=5
        - --audit-log-maxbackup=1
        - --audit-log-maxsize=1024
        ...
        volumeMounts:
          - mountPath: /etc/kubernetes/audit-policy.yaml
            name: audit
            readOnly: true
          - mountPath: /var/log/kubernetes/audit/
            name: audit-log
            readOnly: false
        ...
    ...
    volumes:
    - name: audit
      hostPath:
        path: /etc/kubernetes/audit-policy.yaml
        type: File
    - name: audit-log
      hostPath:
        path: /var/log/kubernetes/audit/
        type: DirectoryOrCreate
    ...
~~~











### etcd

~~~sh
ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
get /registry/secrets/"namespace"/"secretName" | hexdump -C 

~~~

~~~
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: <BASE 64 ENCODED SECRET>
      - identity: {}
~~~

~~~
head -c 32 /dev/urandom | base64 # 이거 결과를 위에 secret 에 넣음
~~~

### apiserver 수정

~~~
- --encryption-provider-config=/etc/kubernetes/enc/enc.yaml  # add this line

volumeMounts:
- name: enc                           # add this line
  mountPath: /etc/kubernetes/enc      # add this line
  readonly: true                      # add this line
      
volumes:
- name: enc                             # add this line
  hostPath:                             # add this line
    path: /etc/kubernetes/enc           # add this line
    type: DirectoryOrCreate             # add this line
~~~

~~~
kubectl get secrets --all-namespaces -o json | kubectl replace -f - # 이전 오브젝트에도 적용할 수 있다.
~~~





### account + role

~~~
k create clusterrolebinding pink-role-binding-cka24-arch --clusterrole=pink-role-cka24-arch --serviceaccount=default:pink-sa-cka24-arch
~~~

~~~
kubectl auth can-i get deployments --as=system:serviceaccount:default:deploy-cka20-arch
yes
~~~

~~~
kubectl create clusterrole pink-role-cka24-arch --resource=* --verb=* # 전체퀀한
~~~

- `kubectl create role test-role -n dev --verb=get,list,watch,create --resource=pods --resource=pods,deployments,services,configmaps`

- `kubectl create rolebinding cluster-test-binding -n dev --role=test-role --serviceaccount=dev:test-svc`





'

'



## 1. secret ????

a. Retrieve the content of the existing secret named default-token-xxxxx in the testing namespace.
   Store the value of the token in thetoken.txt
 b. Create a new secret named test-db-secret in the DB namespace with the following content:
 username: mysql
 password: password@123
 Create the Pod name test-db-pod of image nginx in the namespace db that can accesstest-db-secret via a volume at path /etc/mysql-credentials

~~~sh
To add a Kubernetes cluster to your project, group, or instance:
Navigate to your:
Click Add Kubernetes cluster.
Click the Add existing cluster tab and fill in the details:
Get the API URL by running this command:

$ kubectl cluster-info | grep-E'Kubernetes master|Kubernetes control plane'| awk'/http/ {print $NF}'
uk.co.certification.simulator.questionpool.PList@dd80600

$ kubectl get secret <secret name> -o jsonpath="{['data']['ca\.crt']}"
~~~





## 2. RuntimeClass - no

Create a RuntimeClass named untrusted using the prepared runtime handler named runsc. 
Create a Pods of image alpine:3.13.2 in the Namespace default to run on the gVisor runtime class.
Verify: Exec the pods and run the dmesg, you will see output like this:-

Answer: Send us your feedback on it.

~~~sh
k -n team-purple exec gvisor-test -- dmesg 
k -n team-purple exec gvisor-test > /opt/course/10/gvisor-test-dmesg -- dmesg
~~~





## 3. immutable ?????

Inspect Pods running in **namespace** **prod** and **delete** any Pod that is either not stateless or not immutable.
Use the following strict interpretation of stateless and immutable:

1. Pods being able to store data inside containers must be treated as not stateless.
    Note: You don't have to worry whether data is actually stored inside containers or not already.

2. Pods being configured to be **privileged** in any way must be treated as potentially not stateless or not immutable.

~~~sh
 k get pod db -n prod -o yaml | grep -E 'privileged|RootFilesystem'
~~~





## 4. Trivy - no

Given:
 You may use Trivy's documentation.
Task:
 Use the **Trivy** open-source container scanner to detect images with severe vulnerabilities used by Pods in the **namespace nato**.
 Look for images with **High or Critical severity vulnerabilities** and **delete the Pods that use those images**. Trivy is pre-installed on the cluster's master node. Use cluster's master node to use Trivy.



## 5. AppArmor - no

~~~yaml
On the Cluster worker node, enforce the prepared AppArmor profile
✑ #include<tunables/global> ✑
✑ profilenginx-deny flags=(attach_disconnected) {
✑ #include<abstractions/base> ✑
✑ file, ✑
✑ # Deny all file writes.
✑ deny/** w,
✑}
✑ EOF

Edit the prepared manifest file to include the AppArmor profile.
✑ apiVersion: v1
✑ kind: Pod
✑ metadata:
✑ name:apparmor-pod
✑ spec:
✑ containers:
✑ - name: apparmor-pod
✑ image: nginx
# Finally, apply the manifests files and create the Pod specified on it. Verify: Try to make a file inside the directory which is restricted.
~~~



## 6. falco

Given: You may use Sysdig or Falco documentation.
 Task:
 Use detection tools to detect anomalies like processes spawning and executing something weird frequently in the single container belonging to Pod tomcat.
 Two tools are available to use:

1. falco
2. sysdig

 Tools are pre-installed on the worker1 node only.
 Analyse the container’s behaviour for at least 40 seconds, using filters that detect newly spawning and executing processes.
 Store an incident file at /home/cert_masters/report, in the following format: [timestamp],[uid],[processName]
 Note: Make sure to store incident file on the cluster's worker node, don't move it to master node.

~~~yaml
$ vim /etc/falco/falco_rules.local.yaml
✑ uk.co.certification.simulator.questionpool.PList@dd92f60
$ kill -1 <PID of falco> 

Explanation
$ ssh node01
$ vim /etc/falco/falco_rules.yaml
search for Container Drift Detected & paste in falco_rules.local.yaml

$ vim /etc/falco/falco_rules.local.yaml

- rule: Container Drift Detected (open+create)
desc: New executable created in a container due to open+create
condition: >
  evt.type in (open,openat,creat) and
  evt.is_open_exec=true and
  container and
  not runc_writing_exec_fifo and
  not runc_writing_var_lib_docker and
  not user_known_container_drift_activities and
  evt.rawres>=0
output: >
  %evt.time,%user.uid,%proc.name # Add this/Refer falco documentation priority: ERROR

vim /etc/falco/falco.yaml
~~~



## 7.  Kube-Bench

Fix all issues via configuration and restart the affected components to ensure the new setting takes effect.
Fix all of the following violations that were found against the **API server**:-
 ✑ a. Ensure that the **RotateKubeletServerCertificate** argumentissettotrue.
 ✑ b. Ensure that the **admission control plugin PodSecurityPolicy**isset.
 ✑ c. Ensure that the **--kubelet-certificate-authority** argumentissetasappropriate.

 Fix all of the following violations that were found against the **Kubelet**:-
 ✑ a. Ensure the --anonymous-auth argumentissettofalse.
 ✑ b. Ensure that the --authorization-mode argumentissetto Webhook.

 Fix all of the following violations that were found against the **ETCD**:-
 ✑ a. Ensure that the **--auto-tls** argumentisnotsettotrue
 ✑ b. Ensure that the **--peer-auto-tls** argumentisnotsettotrue
 Hint: Take the use of Tool Kube-Bench

~~~sh
Fix all of thefollowing violations that were found against the API server:-
✑ a. Ensure that the RotateKubeletServerCertificate argumentissettotrue.

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
labels:
  component: kubelet
  tier: control-plane
  name: kubelet
namespace: kube-system
spec:
containers:
- command:
- kube-controller-manager
+ - --feature-gates=RotateKubeletServerCertificate=true
  image: gcr.io/google_containers/kubelet-amd64:v1.6.0
  livenessProbe: failureThreshold: 8 httpGet:
host: 127.0.0.1
path: /healthz
port: 6443
scheme: HTTPS
initialDelaySeconds: 15
timeoutSeconds: 15
name:kubelet
resources:
requests:
cpu: 250m
volumeMounts:
- mountPath: /etc/kubernetes/ name: k8s
readOnly: true
- mountPath: /etc/ssl/certs name: certs
- mountPath: /etc/pki name:pki hostNetwork: true volumes:
- hostPath:
path: /etc/kubernetes
name: k8s
- hostPath:
path: /etc/ssl/certs
name: certs
- hostPath: path: /etc/pki name: pki

✑ b. Ensure that theadmission control plugin PodSecurityPolicy is set.
audit: "/bin/ps -ef | grep $apiserverbin | grep -v grep" tests:
test_items:
- flag: "--enable-admission-plugins"
compare:
op: has
value:"PodSecurityPolicy"
set: true
remediation: |
Follow the documentation and create Pod Security Policy objects as per your environment. Then, edit the API server pod specification file $apiserverconf
on themaster node and set the --enable-admission-plugins parameter to a
value that includes PodSecurityPolicy :
--enable-admission-plugins=...,PodSecurityPolicy,...
Then restart the API Server.
scored: true

✑ c. Ensure that the --kubelet-certificate-authority argument is set as appropriate.
audit: "/bin/ps -ef | grep $apiserverbin | grep -v grep"
tests:
test_items:
- flag: "--kubelet-certificate-authority" set: true
remediation: |
Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the API server pod specification file $apiserverconf on the master node and set the --kubelet-certificate-authority parameter to the path to the cert file for the certificate authority. --kubelet-certificate-authority=<ca-string>
scored: true

Fix all of the following violations that were found against the ETCD:-
✑ a. Ensurethat the --auto-tls argument is not set to true

Edit the etcd pod specification file $ etcd conf on the masternode and either remove the -- auto-tls parameter or set it to false.--auto-tls=false

✑ b. Ensure that the --peer-auto-tls argumentisnotsettotrue
Edit the etcd pod specification file $etcdconf on the masternode and either remove the -- peer-auto-tls parameter or set it to false.--peer-auto-tls=false
~~~



## 8. apparmor

Given: AppArmor is enabled on the worker1 node.
Task:
On the worker1 node,

1. Enforce the prepared AppArmor profile located at: /etc/apparmor.d/nginx
2. Edit the prepared manifest file located at /home/cert_masters/nginx.yaml to apply the apparmor profile 3. Create the Pod using this manifest

~~~sh
$ ssh worker1
$ apparmor_parser -q /etc/apparmor.d/nginx
$ aa-status | grep nginxnginx-profile-1
$ logout
$ vim nginx-deploy.yaml 
Add these lines under metadata:annotations: # Add this line container.apparmor.security.beta.kubernetes.io/<container-name>: localhost/nginx-profile-1

$ kubectl apply -f nginx-deploy.yaml
Explanation
$ ssh worker1
$ apparmor_parser -q /etc/apparmor.d/nginx
$ aa-status | grep nginx nginx-profile-1
$ logout
$ vim nginx-deploy.yaml
~~~

![스크린샷 2023-08-11 오후 5.05.43](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-11 오후 5.05.43.png)



## 9. Falco - no

Using the runtime detection tool Falco, Analyse the container behavior for at least 30 seconds, 
using filters that detect newly spawning and executing processes store the incident file art /opt/falco-incident.txt, containing the detected incidents. one per line, in the format [timestamp],[uid],[user-name],[processName]

Answer: Send us your suggestion on it.

```yaml
[timestamp],[uid],[user-name],[processName]
: "%evt.time,%user.uid,%user.name,%proc.name"
```



## 10. ServiceAccount

**Create a new ServiceAccount** named backend-sa in the existing namespace default, which has the capability to list the pods inside the namespace default.
**Create a new Pod** named backend-pod in the namespace default, mount the newly created sa backend-sa to the pod, and Verify that the pod is able to list pods.
Ensure that the Pod is running.



~~~yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-robot
automountServiceAccountToken: false
~~~

~~~yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  serviceAccountName: build-robot
  automountServiceAccountToken: false
~~~



## 11. CIS Benchmark

A CIS Benchmark tool was run against the kubeadm created cluster and found multiple issues that must be addressed.
Task:
**Fix all issues** via configuration and restart the affected components to ensure the new settings take effect.
**Fix all of the following violations** that were found against the **API server**:
 1.2.7 authorization-mode argument is not set to AlwaysAllow FAIL
 1.2.8 authorization-mode argument includes Node FAIL
 1.2.7 authorization-mode argument includes RBAC FAIL
**Fix all of the following violations** that were found against the **Kubelet**:
 4.2.1 Ensure that the anonymous-auth argument is set to false FAIL
 4.2.2 authorization-mode argument is not set to AlwaysAllow FAIL (Use Webhook autumn/authz where possible)
**Fix all of the following violations** that were found against **etcd**:
 2.2 Ensure that the client-cert-auth argument is set to true

~~~yaml
$ vim /var/lib/kubelet/config.yaml
 uk.co.certification.simulator.questionpool.PList@e615a40

$ systemctl restart kubelet. # To reload kubelet configssh to master1master1

$ vim /etc/kubernetes/manifests/kube-apiserver.yaml- -- authorization-mode=Node,RBAC
$ vim /etc/kubernetes/manifests/etcd.yaml- --client-cert-auth=true
Explanationssh to worker1

$ vim /var/lib/kubelet/config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
anonymous:
enabled: true #Delete this
enabled: false #Replace by this webhook:
cacheTTL: 0s
enabled: true
x509:
clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
mode: AlwaysAllow #Delete this
mode: Webhook #Replace by this
webhook:
cacheAuthorizedTTL: 0s
cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 10.96.0.10 clusterDomain: cluster.local cpuManagerReconcilePeriod: 0s evictionPressureTransitionPeriod: 0s fileCheckFrequency: 0s healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 0s imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s

worker1 $ systemctl restart kubelet. # To reload kubelet configssh to master1master1 $ vim /etc/kubernetes/manifests/kube-apiserver.yaml
~~~

![스크린샷 2023-08-10 오후 9.05.59](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-10 오후 9.05.59.png)



## 12. Image Policy ?????

A container image scanner is set up on the cluster, but it's not yet fully integrated into the cluster's configuration. When complete, the container image scanner shall scan for and reject the use of vulnerable images.
Task:
 You have to complete the entire task on the cluster's master node, where all services and files have been prepared and placed.
 Given an incomplete configuration in directory /etc/Kubernetes/config and a functional container image scanner with HTTPS endpoint https://imagescanner.local:8181/image_policy:

1. Enable the necessary plugins to **create an image policy**
2. **Validate** the control configuration and change it to an implicit deny
3. **Edit the configuration** to point to the provided HTTPS endpoint correctly

 Finally, test if the configuration is working by trying to deploy the vulnerable resource /home/cert_masters/test-pod.yml
 Note: You can find the container image scanner's log file at /var/log/policy/scanner.log

~~~sh
$ cd /etc/Kubernetes/config1.
Edit kubeconfig to explicity deny
$ vim kubeconfig.json # "defaultAllow": false # Change to false. fix server
parameter by taking its value from ~/.kube/config

[master@cli] $cat /etc/kubernetes/config/kubeconfig.yaml | grep serverserver:3. Enable ImagePolicyWebhook

[master@cli] $ vim /etc/kubernetes/manifests/kube-apiserver.yaml
- --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook # Add
- --admission-control-config-file=/etc/kubernetes/config/kubeconfig.json # Add


Explanation[desk@cli] $ ssh master
[master@cli] $ cd /etc/Kubernetes/config
[master@cli] $ vim kubeconfig.json
{
"imagePolicy": {
"kubeConfigFile": "/etc/kubernetes/config/kubeconfig.yaml", "allowTTL": 50,
"denyTTL": 50,
"retryBackoff": 500,
"defaultAllow": true # Delete this
"defaultAllow": false # Add this
}
}
~~~

![스크린샷 2023-08-10 오후 9.13.32](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-10 오후 9.13.32.png)

![스크린샷 2023-08-10 오후 9.14.17](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-10 오후 9.14.17.png)

Text
 Description automatically generated
 Note: We can see a missing value here, so how from where i can get this value [master@cli] $cat ~/.kube/config | grep serveror[master@cli] $cat /etc/kubernetes/manifests/kube-apiserver.yaml

![스크린샷 2023-08-10 오후 9.14.38](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-10 오후 9.14.38.png)

[master@cli] $vim /etc/kubernetes/config/kubeconfig.yaml

![스크린샷 2023-08-10 오후 9.14.53](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-10 오후 9.14.53.png)



## 13. CSR ?????

**Create a User** named john, create the CSR Request, fetch the certificate of the user after approving it.
**Create a Role** name john-role to list secrets, pods in namespace john
Finally, **Create a RoleBinding** named john-role-binding to attach the newly created role john-role to the user john in the namespace john.
To Verify: Use the kubectl auth CLI command to verify the permissions.

~~~yaml
# se kubectl to create a CSR and approve it.
# Get the list of CSRs:
$ kubectl get csr
#Approve the CSR:
kubectl certificate approve myuser
# Get the certificateRetrieve the certificate from the CSR:
kubectl get csr/myuser -o yaml
# here are the role and role-binding to give john permission to create NEW_CRD resource: kubectl apply-froleBindingJohn.yaml--as=john rolebinding.rbac.authorization.k8s.io/john_external-rosource-rbcreated

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
		name: john_crd
namespace: development-john 
subjects:
-kind:User
name:john apiGroup:rbac.authorization.k8s.io roleRef:
kind:ClusterRole
name:crd-creation

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1 metadata:
name:crd-creation
rules: -apiGroups:["kubernetes-client.io/v1"] resources:["NEW_CRD"] verbs:["create, list, get"]
~~~



## 14. Dockerfile - no

Before Making any changes build the Dockerfile with tag base:v1 Now Analyze and edit the given Dockerfile(based on ubuntu 16:04)
Fixing two instructions present in the file, Check from Security Aspect and Reduce Size point of view.

Dockerfile:

~~~dockerfile
FROM ubuntu:latest
RUN apt-get update -y
RUN apt install nginx -y
COPY entrypoint.sh /
RUN useradd ubuntu
ENTRYPOINT ["/entrypoint.sh"]
USER ubuntu
~~~

~~~
#!/bin/bash
echo "Hello from CKS"
~~~

 After fixing the Dockerfile, build the docker-image with the tag base:v2 To Verify: Check the size of the image before and after the build.

~~~
FROM ubuntu
ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y golang-go
COPY app.go .
RUN CGO_ENABLED=0 go build app.go
CMD ["./app"]
~~~

~~~
FROM ubuntu
ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y golang-go
COPY app.go .
RUN CGO_ENABLED=0 go build app.go

FROM alpine
COPY --from=0 /app .
CMD ["./app"]
~~~











## 15. kube-bench ?????

**Fix all issues** via configuration and restart the affected components to ensure the new setting takes effect.
**Fix all of the following violations** that were found against the **API server**:- 
 a. Ensure the --authorization-mode argument includes RBAC
 b. Ensure the --authorization-mode argument includes Node
 c. Ensure that the --profiling argumentisset to false

**Fix all of the following violations** that were found against the **Kubelet**:- 
 a. Ensure the --anonymous-auth argumentis set tofalse.
 b. Ensure thatthe --authorization-mode argumentis set to Webhook.

**Fix all of the following violations** that were found against the **ETCD**:-
 a. Ensure that the --auto-tls argument is not set to true

~~~sh
API server:
 Ensure the --authorization-mode argument includes RBAC
#Turn on Role Based Access Control.Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode. Fix - BuildtimeKubernetesapiVersion: v1

kind: Pod
metadata:
creationTimestamp: null
labels:
component: kube-apiserver
tier: control-plane
name:kube-apiserver
namespace: kube-system
spec:
containers:
-command:
+ - kube-apiserver
+ - --authorization-mode=RBAC,Node
image: gcr.io/google_containers/kube-apiserver-amd64:v1.6.0
livenessProbe:
failureThreshold:8
httpGet:
host:127.0.0.1
path: /healthz
port:6443
scheme: HTTPS
initialDelaySeconds:15
timeoutSeconds:15
name: kube-apiserver-should-pass
resources:
requests:
cpu: 250m
volumeMounts:
-mountPath: /etc/kubernetes/ name: k8s
readOnly:true
-mountPath: /etc/ssl/certs name: certs
-mountPath: /etc/pki name: pki hostNetwork:true volumes:
-hostPath:
path: /etc/kubernetes
name: k8s
-hostPath:
path: /etc/ssl/certs
name: certs
-hostPath:
path: /etc/pki
name: pki
 Ensure the --authorization-mode argument includes Node
#Remediation: Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --authorization-mode parameter to a value that includes Node. --authorization-mode=Node,RBAC

Audit:
/bin/ps -ef | grep kube-apiserver | grep -v grep
Expected result:
'Node,RBAC' has 'Node'
 Ensure that the --profiling argumentissettofalse
#Remediation: Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the below parameter.
--profiling=false

Audit:
/bin/ps -ef | grep kube-apiserver | grep -v grep
Expected result:
'false' is equal to 'false'

Fix all of the following violations that were found against the Kubelet:-
 uk.co.certification.simulator.questionpool.PList@db811b0

# Remediation: If using a Kubelet config file, edit the file to set authentication: anonymous: enabled to false. If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.
--anonymous-auth=false

# Based on your system, restart the kubelet service. For example:
systemctl daemon-reload
systemctl restart kubelet.service
~~~



~~~sh
Audit:
/bin/ps -fC kubelet
Audit Config:
/bin/cat /var/lib/kubelet/config.yaml
Expected result:
 'false' is equal to 'false'

2) Ensure that the --authorization-mode argumentis set to Webhook.
Audit
docker inspect kubelet | jq -e'.[0].Args[] |match("--authorization-mode=Webhook").string'
Returned Value: --authorization-mode=Webhook

Fix all of the following violations that were found against the ETCD:-a. Ensure that the --auto-tls argument is not set to true
Do not use self-signed certificates for TLS. etcd is a highly-available key value store used
by Kubernetes deployments for persistent storage of all of its REST API objects. These
objects are sensitive in nature and should not be available to unauthenticated clients.You
should enable the client authentication via valid certificates to secure the access to the etcd
service.
Fix - BuildtimeKubernetes

apiVersion: v1
kind: Pod
metadata:
annotations:
scheduler.alpha.kubernetes.io/critical-pod:""
creationTimestamp: null
labels:
component: etcd
tier: control-plane
name: etcd
namespace: kube-system
spec:
containers:
-command:
+ - etcd
+ - --auto-tls=true
image:k8s.gcr.io/etcd-amd64:3.2.18
imagePullPolicy: IfNotPresent
livenessProbe: exec:
command: - /bin/sh
- -ec
- ETCDCTL_API=3 etcdctl --endpoints=https://[192.168.22.9]:2379 -- cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt -- key=/etc/kubernetes/pki/etcd/healthcheck-client.key
get foo failureThreshold:8 initialDelaySeconds:15 timeoutSeconds:15 name: etcd-should-fail resources: {} volumeMounts: -mountPath: /var/lib/etcd name: etcd-data
-mountPath: /etc/kubernetes/pki/etcd
name: etcd-certs hostNetwork:true priorityClassName: system-cluster-critical volumes:
-hostPath: path:/var/lib/etcd type: DirectoryOrCreate name: etcd-data -hostPath:
path: /etc/kubernetes/pki/etcd type: DirectoryOrCreate name: etcd-certs
status: {}
~~~



## 16. audit - 답이상함 ????? 

Enable audit logs in the cluster, To Do so, enable the log backend, and ensure that

1. logs are stored at /var/log/kubernetes-logs.txt.
2. Log files are retained for 12 days.
3. at maximum, a number of 8 old audit logs files are retained.
4. set the maximum size before getting rotated to 200MB

 Edit and extend the **basic policy** to log:

1. namespaces changes at **RequestResponse**
2. Log the request body of **secrets** changes in the namespace kube-system.
3. Log all other resources in core and extensions at the Request level.
4. Log "pods/portforward", "services/proxy" at **Metadata** level.
5. Omit the Stage **RequestReceived**

 All other requests at the Metadata level

~~~sh
Kubernetes auditing provides a security-relevant chronological set of records about a cluster. Kube-apiserver performs auditing. Each request on each stage of its execution generates an event, which is then pre-processed accordingto a certain policy and written to a backend. The policy determines what’s recorded and the backends persist the records. You might want to configure the audit log as part of compliance with the CIS (Center for Internet Security) Kubernetes Benchmark controls.
The audit log can be enabled by default using the following configuration in cluster.yml:
services:
kube-api:
audit_log:
enabled:true

When the audit log is enabled, you should be able to see the default values at /etc/kubernetes/audit-policy.yaml
The log backend writes audit events to a file in JSONlines format. You can configure the log audit backend using the following kube-apiserver flags:
 --audit-log-path specifies the log file path that log backend uses to write audit events. Not specifying this flag disables log backend. - means standard out
 --audit-log-maxage defined the maximum number of days to retain old audit log files
 --audit-log-maxbackup defines the maximum number of audit log files to retain
 --audit-log-maxsize defines the maximum size in megabytes of the audit log file
before it gets rotated
If your cluster's control plane runs the kube-apiserver as a Pod, rememberto mount
the hostPath to the location of the policy file and log file, so that audit records are persisted.

For example:
--audit-policy-file=/etc/kubernetes/audit-policy.yaml
--audit-log-path=/var/log/audit.log
~~~



## 17. PodSecurityPolicy ?????

**Create a PSP** that will only allow the persistent volume claim as the volume type in the namespace restricted.
**Create a new PodSecurityPolicy** named prevent-volume-policy which prevents the pods which is having different volumes mount apart from persistent volume claim.
**Create a new ServiceAccount** named psp-sa in the namespace restricted.
 **Create a new ClusterRole** named psp-role, which uses the newly created Pod Security Policy prevent-volume-policy
 **Create a new ClusterRoleBinding** named psp-role-binding, which binds the created ClusterRole psp-role tothe created SA psp-sa.
 Hint:
 Also, Check the Configuration is working or not by trying to Mount a Secret in the pod maifest, it should get failed.
 POD Manifest:

~~~yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default'
    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'        
    seccomp.security.alpha.kubernetes.io/defaultProfileName: 'runtime/default'     
    apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'
spec:
  privileged: false
# Required to prevent escalations to root. allowPrivilegeEscalation: false
# This is redundant with non-root + disallow privilege escalation,
# butwe can provide it for defense in depth. requiredDropCapabilities: - ALL
# Allow core volume types.
  volumes:
  - 'configMap' - 'emptyDir' - 'projected' - 'secret'
  - 'downwardAPI'
# Assume that persistentVolumes set up by thecluster admin are safe to use.
  - 'persistentVolumeClaim'
  hostNetwork: false hostIPC: false
    hostPID: false runAsUser:
# Require the container to run without root privileges. rule: 'MustRunAsNonRoot'
  seLinux:
# This policyassumes the nodes are using AppArmor rather than SELinux. rule: 'RunAsAny' supplementalGroups: rule: 'MustRunAs' ranges:
# Forbid adding the root group.
- min: 1
max: 65535
fsGroup:
rule: 'MustRunAs'
ranges:
# Forbid adding the root group. - min: 1
max: 65535
readOnlyRootFilesystem: false
~~~



## 18. ServiceAccount - no

Given an existing Pod named nginx-pod running in the namespace test-system, **fetch the service-account-name** used and put the content in /candidate/KSC00124.txt
**Create a new Role** named dev-test-role in the namespace test-system, which can perform update operations, on resources of type namespaces.
 **Create a new RoleBinding** named dev-test-role-binding, which binds the newlycreated Role to the Pod's ServiceAccount ( found in the Nginx pod running in namespace test-system).



## 19. Falco - no xxxxxxxx

Using the runtime detection tool Falco, Analyse the container behavior for at least 20 seconds, using filters that detect newly spawning and executing processes in asingle container of Nginx.
store the incident file art /opt/falco-incident.txt, containing the detected incidents. one per line, in the format
[timestamp],[uid],[processName]



## 20. ETCD Encript - no ?????

Secrets stored in the etcd is not secure at rest, you can use the etcdctl command utility to find the secret value
 for e.g:-
 ETCDCTL_API=3 etcdctl get /registry/secrets/default/cks-secret --cacert="ca.crt" -- cert="server.crt" --key="server.key"

Output

![스크린샷 2023-08-10 오후 11.59.00](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-10 오후 11.59.00.png)

Using the Encryption Configuration, **Create the manifest,** which secures the resource secrets using the provider AES-CBC and identity, to encrypt the secret-data at rest and ensure all secrets are encrypted with the new configuration.









## 21. NetworkPolicy ?????

**Create a network policy** named allow-np, that allows pod in the namespace staging to connect to port 80 of other pods in the same namespace.
 Ensure that Network Policy:-

1. Does not allow access to pod not listening on port 80.
2. Does not allow access from Pods, not in namespace staging.

~~~yaml
apiVersion:networking.k8s.io/v1
kind:NetworkPolicy
metadata:
name:network-policy
spec:
podSelector:{} #selects all the pods in thenamespace deployed
policyTypes:
-Ingress
ingress:
-ports:#in input traffic allowed only through 80 port only
-protocol:TCP
port:80
~~~



## 22. port - no ?????

Service is running on port 389 inside the system, find the process-id of the process, and stores the names of all the open-files inside the /candidate/KH77539/files.txt, and also delete the binary.



## 23. NetworkPolicy ????

 Task:
 Create a NetworkPolicy named **restricted-policy t**o restrict access to Pod product running in namespace **dev**.
 Only allow the following Pods to connect to Pod products-service:

1. Pods in the namespace qa
2. Pods with label environment: stage, in any namespace

~~~yaml
Explanation:
$ k get ns qa --show-labels
 uk.co.certification.simulator.questionpool.PList@dd83920

$ k get pods -n dev --show-labels
 uk.co.certification.simulator.questionpool.PList@dd83a10

$ k get ns qa --show-labels
NAME STATUS AGE LABELS
qa Active 47m env=stage

k get pods -n dev --show-labels
NAME READY STATUS RESTARTS AGE LABELS product 1/1 Running 0 3s env=dev-team


$ vim netpol2.yaml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: restricted-policy
  namespace: dev
spec:
  podSelector:
    matchLabels:
      env: dev-team
  policyTypes:
    - Ingress
  ingress:
    - from:
      - namespaceSelector:
          matchLabels: env: stage
      - podSelector:
          matchLabels: env: stage
~~~



## 24. Dockerfile - no ?????

~~~
Analyze and edit the given Dockerfile

FROM ubuntu:latest
RUN apt-getupdate -y
RUN apt-install nginx -y 
COPY entrypoint.sh / 
ENTRYPOINT ["/entrypoint.sh"] 
USER ROOT

Fixing two instructions present in the file being prominent security bestpractice issues
Analyze and edit the deployment manifest file

apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo-2
spec:
  securityContext:
    runAsUser: 1000
  containers:
  - name: sec-ctx-demo-2
    image:gcr.io/google-samples/node-hello:1.0
    securityContext:
      runAsUser: 0
      privileged:True
  allowPrivilegeEscalation:false

Fixing two fields present in the file being prominent security best practice issues
Don't add or remove configuration settings; only modify the existing configuration settings
Whenever you need an unprivileged user for any of the tasks, use user test-user with the user id 5487
~~~





## 25. audit ?????

Task: Enable audit logs in the cluster.
 To do so, enable the log backend, and ensure that:

1. logs are stored at /var/log/Kubernetes/logs.txt
2. log files are retained for 5 days
3. at maximum, a number of 10 old audit log files are retained

 A basic policy is provided at /etc/Kubernetes/logpolicy/audit-policy.yaml. 
 It only specifies what not to log.
 Note: The **base policy** is located on the cluster's master node.
 Edit and extend the basic policy to log:

1. Nodes changes at **RequestResponse** level
2. The request body of persistent volumes changes in the namespace frontend
3. ConfigMap and Secret changes in all namespaces at the Metadata level Also, add a catch-all rule to log all other requests at the Metadata level

~~~sh
$ vim /etc/kubernetes/log-policy/audit-policy.yaml
 uk.co.certification.simulator.questionpool.PList@dd91c30
$ vim /etc/kubernetes/manifests/kube-apiserver.yaml
Add these uk.co.certification.simulator.questionpool.PList@dd91f20
- --audit-log-maxbackup=10

$ ssh master1
$ vim /etc/kubernetes/log-policy/audit-policy.yaml

apiVersion: audit.k8s.io/v1 # This is required.
kind: Policy
# Don't generate audit events for all requests in RequestReceived stage.
omitStages:
- "RequestReceived" rules:
# Don't log watch requests by the "system:kube-proxy" on endpoints or services - 
level: None
users: ["system:kube-proxy"]
verbs: ["watch"] resources:
- group: "" # core API group
resources: ["endpoints", "services"]
# Don't log authenticated requests to certain non-resource URL paths.
- level: None
userGroups: ["system:authenticated"]
nonResourceURLs:
- "/api*" # Wildcard matching.
- "/version"
# Add your changes below



- level: RequestResponse
userGroups: ["system:nodes"] # Block for nodes

- level: Request
resources:
- group: "" # core API group
resources: ["persistentvolumes"] # Block for persistentvolumes
namespaces: ["frontend"] # Block for persistentvolumes of frontend ns


- level: Metadata
resources:
- group: "" # core API group
resources: ["configmaps", "secrets"] # Block for configmaps & secrets
- level: Metadata # Block for everything else


$ vim /etc/kubernetes/manifests/kube-apiserver.yaml

apiVersion: v1
kind: Pod
metadata:
annotations:
  kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.0.0.5:6443
labels:
component: kube-apiserver
tier: control-plane
name: kube-apiserver
namespace: kube-system
spec:
containers:
- command:
- kube-apiserver
- --advertise-address=10.0.0.5
- --allow-privileged=true
- --authorization-mode=Node,RBAC
- --audit-policy-file=/etc/kubernetes/log-policy/audit-policy.yaml #Add this
- --audit-log-path=/var/log/kubernetes/logs.txt #Add this
- --audit-log-maxage=5 #Add this
- --audit-log-maxbackup=10 #Add this
output truncated

~~~







## 26. ServiceAccount ?????

A pod fails to run because of an incorrectly specified ServiceAccount

Task:
 **Create a new service account** named backend-qa in an existing namespace qa, which must not have access to any secret.
 **Edit the frontend** pod yaml to use backend-qa service account

Note: You can find the frontend pod yaml at /home/cert_masters/frontend-pod.yaml

~~~sh
$ k create sa backend-qa -n qasa/backend-qa

$ k get role,rolebinding -n qa
resources found in qa namespace.

$ k create role backend -n qa --resource pods,namespaces,configmaps --verb list # No access to secret 
$ k create rolebinding backend -n qa --role backend --serviceaccount qa:backend-qa

$ vim /home/cert_masters/frontend-pod.yaml
 uk.co.certification.simulator.questionpool.PList@dd92a20

$ k apply -f /home/cert_masters/frontend-pod.yaml



$ k create sa backend-qa -n qaserviceaccount/backend-qa
created
$ k get role,rolebinding -n qaNo
resources found in qa namespace.

$ k create role backend -n qa --resource pods,namespaces,configmaps --verb listrole.rbac.authorization.k8s.io/backend
created
$ k create rolebinding backend -n qa --role backend --serviceaccount qa:backend-qarolebinding.rbac.authorization.k8s.io/backend
created
$ vim /home/cert_masters/frontend-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  serviceAccountName: backend-qa # Add this
  image: nginx
  name: frontend

$ k apply -f /home/cert_masters/frontend-pod.yamlpod/frontend createdhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
~~~





## 27. kubesec - no

Use the kubesec docker images to scan the given YAML manifest, edit and apply the advised changes, and passed with a score of 4 points.
 kubesec-test.yaml

~~~yaml
apiVersion: v1
kind: Pod
metadata:
name: kubesec-demo
spec:
containers:
- name: kubesec-demo
image: gcr.io/google-samples/node-hello:1.0
securityContext:
  readOnlyRootFilesystem: true

# Hint: docker run -i kubesec/kubesec:512c5e0 scan /dev/stdin <kubesec-test.yaml
~~~







## 28. NetworkPolicy

Create a new NetworkPolicy named deny-all in the namespace testing
which denies all traffic of type ingress and egress traffic

~~~yaml
# You can create a "default" isolation policyfor a namespace by creating a NetworkPolicy that selects all pods but does not allow any ingress traffic to those pods. ---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
name: default-deny-ingress
spec:
podSelector: {}
policyTypes:
- Ingress
#You can create a "default" egress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any egress traffic from those pods.
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress
spec:
podSelector: {}
egress:
- {}
policyTypes:
- Egress

# Default deny all ingress and all egress trafficYou can create a "default" policy for a namespace which # # # prevents all ingress AND egress traffic bycreating the following NetworkPolicy in that namespace.
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
podSelector: {} 
policyTypes:
- Ingress
- Egress
# This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed ingress or egress traffic.
~~~







## 29.  audit - no ?????

Enable audit logs in the cluster, To Do so, enable the log backend, and ensure that
1. logs are stored at /var/log/kubernetes/kubernetes-logs.txt.
2. Log files are retained for 5 days.
3. at maximum, a number of 10 old audit logs files are retained. 

Edit and extend the basic policy to log:

1. Cronjobs changes at RequestResponse
2. Log the request body of deployments changesinthenamespacekube-system.
3. Log all other resources in core and extensions at the Request level.
4. Don't log watch requests by the "system:kube-proxy" on endpoints or



## 30. RuntimeClass

This cluster has been prepared to support runtime handler, runsc as well as traditional one.
Task:
**Create a RuntimeClass** named not-trusted using the prepared runtime handler names **runsc**.
**Update all Pods** in the namespace server to run on new runtime.

~~~sh
Find all the pods/deployment and edit runtimeClassName parameter to not-trusted under 

$ k edit deploy nginx
 uk.co.certification.simulator.questionpool.PList@e614c40

$ vim runtime.yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: not-trusted
handler: runsc

$ k apply -f runtime.yaml
$ k get pods
NAME READY STATUS RESTARTS AGE nginx-6798fc88e8-chp6r 1/1 Running 0 11m nginx-6798fc88e8-fs53n 1/1 Running 0 11m nginx-6798fc88e8-ndved 1/1 Running 0 11m

$ k get deploy
NAME READY UP-TO-DATE AVAILABLE AGE nginx 3/3 11 3 5m [desk@cli]
k edit deploy nginx
~~~

![스크린샷 2023-08-10 오후 8.14.08](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-10 오후 8.14.08.png)



## 31. Secret 

Task:
Retrieve the content of the existing secret named adam in the safe namespace.
Store the username field in a file names /home/cert-masters/username.txt, and the password field in a file named /home/cert-masters/password.txt.

1. You must create both files; they don't exist yet.
2. Do not use/modify the created files in the following steps, create new temporary files if needed.

 **Create a new secret** names newsecret in the safe namespace, with the following content:
 Username: dbadmin
 Password: moresecurepas

Finally, create a new Pod that has access to the secret newsecret via a volume:
Namespace:safe
Pod name:mysecret-pod
Container name:db-container
Image:redis
Volume name:secret-vol
Mount path:/etc/mysecret



~~~sh
1. Get the secret, decrypt it & save in filesk get secret adam -n safe -o yaml
2. Create new secret using --from-literal
$ k create secret generic newsecret -n safe --from-literal=username=dbadmin --from-literal=password=moresecurepass
3. Mount it as volume of db-container of mysecret-pod
$ k get secret adam -n safe -o yaml
~~~

![스크린샷 2023-08-10 오후 7.50.22](/Users/staek/Library/Application Support/typora-user-images/스크린샷 2023-08-10 오후 7.50.22.png)

~~~yaml
$ echo "Y2VydC1tYXN0ZXJz" | base64 -d | tee -a /home/cert-masters/username.txt

$ echo "c2VjcmV0cGFzcw==" | base64 -d | tee -a /home/cert-masters/password.txt
$ k create secret generic newsecret -n safe --from-literal=username=dbadmin -- from-literal=password=moresecurepasssecret/newsecret
$ vim /home/certs_masters/secret-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: mysecret-pod
  namespace: safe
  labels:
    run: mysecret-pod
spec:
  containers:
  - name: db-container
    image: redis
    volumeMounts:
    - name: secret-vol
      mountPath: /etc/mysecret
      readOnly: true
  volumes:
  - name: secret-vol
    secret:
      secretName: newsecret
~~~



~~~sh
$ k apply -f /home/certs_masters/secret-pod.yamlpod/mysecret-pod
$ k exec -it mysecret-pod -n safe – cat /etc/mysecret/usernamedbadmin

$ k exec -it mysecret-pod -n safe – cat /etc/mysecret/passwordmoresecurepas
~~~





## 32. PodSecurityPolicy ?????????

**Create a PSP** that will prevent the creation ofprivileged pods in the namespace.
**Create a new PodSecurityPolicy** named prevent-privileged-policy which prevents the creation of privileged pods.
**Create a new ServiceAccount** named psp-sa in the namespace default.
**Create a new ClusterRole** namedprevent-role, which uses the newly created Pod Security Policy prevent-privileged-policy.
**Create a new ClusterRoleBinding** named prevent-role-binding, which binds the created ClusterRole prevent-role to the created SA psp-sa.
Also, Check the Configuration is working or not by trying to **Create a Privileged pod**, it should get failed.

~~~yaml
Create a PSP that will prevent the creation of privileged pods in the namespace.
$ cat clusterrole-use-privileged.yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: use-privileged-psp
rules:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs: ['use']
  resourceNames:
  - default-psp
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
name: privileged-role-bind
namespace: psp-test roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: use-privileged-psp subjects:
- kind: ServiceAccount
name: privileged-sa

$ kubectl -n psp-test apply -f clusterrole-use-privileged.yaml
# After a few moments, the privileged Pod should be created.

# Create a new PodSecurityPolicy named prevent-privileged-policy which prevents the creation ofprivileged pods.

apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
name: example
spec:
  privileged: false # Don't allow privileged pods!
  # The rest fills in some required fields. seLinux:
  rule: RunAsAny 
  supplementalGroups: 
  rule: RunAsAny 
  runAsUser:
  rule: RunAsAny
  fsGroup:
  rule: RunAsAny
  volumes:
  - '*'

And create it with kubectl:
kubectl-admin create -f example-psp.yaml
Now, as the unprivileged user, try to create a simple pod:

kubectl-user create -f-<<EOF
apiVersion: v1
kind: Pod
metadata:
name: pause
spec:
containers:
- name: pause
image: k8s.gcr.io/pause
EOF

# The output is similar to this:
# Error from server (Forbidden): error when creating "STDIN": pods "pause" isforbidden: unable to validate against any pod security policy: []
#Create a new ServiceAccount named psp-sa in the namespace default.

$ cat clusterrole-use-privileged.yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: use-privileged-psp
rules:
- apiGroups: ['policy']
resources: ['podsecuritypolicies']
verbs: ['use']
resourceNames:
- default-psp
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: privileged-role-bind
  namespace: psp-test
  roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: use-privileged-psp
subjects:
- kind: ServiceAccount
  name: privileged-sa

$ kubectl -n psp-test apply -fclusterrole-use-privileged.yaml
# After a few moments, the privileged Pod should be created.
#Create a new ClusterRole named prevent-role, which uses the newly created Pod Security Policy prevent-privileged-policy.

apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
name: example
spec:
privileged: false # Don't allow privileged pods!
# The rest fills in some required fields. seLinux:
rule:RunAsAny 
supplementalGroups: 
rule:RunAsAny
runAsUser:
rule:RunAsAny
fsGroup:
rule:RunAsAny
volumes:
-'*'

And create it with kubectl: kubectl-admin create -f example-psp.yaml
Now, as the unprivileged user, try to create a simple pod:

kubectl-user create -f-<<EOF
apiVersion: v1
kind: Pod metadata: name:pause spec: containers:
- name: pause
image: k8s.gcr.io/pause
EOF

The output is similar to this:
Error from server (Forbidden): error when creating "STDIN": pods "pause" is forbidden:
unable to validate against any pod security policy: []
 Create a new ClusterRoleBinding named prevent-role-binding, which binds the created ClusterRole prevent-role to the created SA psp-sa.

apiVersion: rbac.authorization.k8s.io/v1
# This role binding allows "jane" to read pods in the "default" namespace.
# You need to already have a Role named "pod-reader" in that namespace.
kind: RoleBinding
metadata:
  name:read-pods
  namespace:default
subjects:
# You can specify more than one "subject" -kind:User
name: jane # "name" is case sensitive apiGroup:rbac.authorization.k8s.io roleRef:
# "roleRef" specifies the binding to a Role / ClusterRole kind:Role#this must be Role or ClusterRole name:pod-reader# this must match the name of the Role or ClusterRole you wish to bind to apiGroup:rbac.authorization.k8s.io

apiVersion:rbac.authorization.k8s.io/v1
kind:Role
metadata:
  namespace:default
  name:pod-reader
  rules:
  - apiGroups:[""] # "" indicates the core API group
    resources:["pods"]
    verbs:["get","watch","list"]
~~~





## 33. Image Policy - no xxxxxx

A container image scanner is set up on the cluster.
 Given an incomplete configuration in thedirectory
 /etc/kubernetes/confcontrol and a functional container image scanner with HTTPS endpoint https://test-server.local.8081/image_policy

1. Enable the admission plugin.
2. Validate the control configuration and change it to implicit deny.

 Finally,test the configuration by deploying the pod having the image tag as latest.



## 34. PodSecurityPolcy

A **PodSecurityPolicy** shall prevent the creation of privileged Pods in a specific namespace.

1. **Create a new PodSecurityPolcy** named deny-policy, which prevents the creation of privileged Pods.
2. **Create a new ClusterRole** name deny-access-role, which uses the newly created PodSecurityPolicy deny-policy.
3. **Create a new ServiceAccount n**amed psd-denial-sa in the existing namespace development. Finally, create a new ClusterRoleBindind named restrict-access-bind, which binds the newly created ClusterRole deny-access-role to the newly created ServiceAccount psp-denial-sa

~~~yaml
Create psp to disallow privileged container
 uk.co.certification.simulator.questionpool.PList@dd90cb0

k create sa psp-denial-sa -n development
 uk.co.certification.simulator.questionpool.PList@dd90eb0 namespace: development Explanationmaster1

$ vim psp.yaml

apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: deny-policy
spec:
  privileged: false # Don't allow privileged pods!
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
 runAsUser:
 rule: RunAsAny
 fsGroup:
 rule: RunAsAny
 volumes:
 - '*'

$ vim cr1.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: deny-access-role
rules:
- apiGroups: ['policy']
resources: ['podsecuritypolicies']
verbs: ['use']
resourceNames:
- “deny-policy”

$ k create sa psp-denial-sa -n development
$ vim cb1.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
name: restrict-access-bing
roleRef:
kind: ClusterRole
name: deny-access-role
apiGroup: rbac.authorization.k8s.io
subjects
# Authorize specific
service accounts:
- kind: ServiceAccount
  name: psp-denial-sa
namespace: development

~~~





## 35. ServiceAccount ?????

 A Role bound to a Pod's ServiceAccount grants overly permissive permissions. Complete the following tasks to reduce the set of permissions.
 Task:
 Given an existing Pod named web-pod running in the namespace database.

1. **Edit the existing Role bound to the Pod's ServiceAccountest-sa** to only allow performing get operations, only on resources of type Pods.
2. **Create a new Role** named test-role-2 in the namespace database, which only allows performing update operations, only on resources of type statuefulsets.
3. **Create a new RoleBinding** named test-role-2-bind binding the newly created Role to the Pod's ServiceAccount.

~~~yaml
$ k edit role test-role -n database
 uk.co.certification.simulator.questionpool.PList@e615200

$ k create role test-role-2 -n database --resource statefulset --verb update
$ k create rolebinding test-role-2-bind -n database --role test-role-2 --serviceaccount=database:test-sa

$ k get pods -n database
NAME READY STATUS RESTARTS AGE LABELS web-pod 1/1 Running 0 34s run=web-pod
$ k get roles -n databasetest-role[desk@cli]$ k edit role test-role -n database

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
creationTimestamp: "2021-06-13T11:12:23Z"
name: test-role
namespace: database
resourceVersion: "1139"
selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/database/roles/test-role
uid: 49949265-6e01-499c-94ac-5011d6f6a353 rules:
- apiGroups:
- ""
resources:
- pods verbs:
- "*" # Delete this
- get # Replace by this
~~~



## 36. Trivy - no xxxxxx

use the Trivy to scan the following images,

1. amazonlinux:1
2. k8s.gcr.io/kube-controller-manager:v1.18.6

 Look for images with HIGH or CRITICAL severity vulnerabilities and store theoutput of the same in /opt/trivy-vulnerable.txt



## 37. Dockerfile Security

Task:
Analyse and **edit the given Dockerfile** (based on the ubuntu:18:04 image)
/home/cert_masters/Dockerfile fixing two instructions present in the file being prominent security/best-practice issues.
Analyse and edit the given manifest file /home/cert_masters/mydeployment.
yaml **fixing two fields** present in the file being prominent security/best-practice issues.

Note: Don't add or remove configuration settings; only modify the existing configuration settings, so that two configuration settings each are no longer security/best-practice concerns.
 Should you need an unprivileged user for any of the tasks, use user nobody with user id 65535



1. For Dockerfile: Fix the image version & user name in Dockerfile
2. For mydeployment.yaml : 

~~~
# /home/cert_masters/Dockerfile
FROM ubuntu:latest # Remove this
FROM ubuntu:18.04 # Add this
USER root # Remove this
USER nobody # Add this
RUN apt get install -y lsof=4.72 wget=1.17.1 nginx=4.2
ENV ENVIRONMENT=testing
USER root # Remove this
USER nobody # Add this
CMD ["nginx -d"]
~~~

~~~yaml
$ vim /home/cert_masters/mydeployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: kafka
    name: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: kafka
    spec:
      containers:
      - image: bitnami/kafka 
        name: kafka 
        volumeMounts:
        - name: kafka-vol
          mountPath: /var/lib/kafka
        securityContext:
          {"capabilities":{"add":["NET_ADMIN"],"drop":["all"]},"privileged": True,"readOnlyRootFilesystem": False, "runAsUser": 65535} # Delete This
          {"capabilities":{"add":["NET_ADMIN"],"drop":["all"]},"privileged": False,"readOnlyRootFilesystem": True, "runAsUser": 65535} # Add This resources: {}
      volumes:
        - name: kafka-vol
          emptyDir: {}
status: {}
~~~

- privileged : false
- readOnlyRootFilesystem : true









## 38. AppArmor - no xxxxxx

On the Cluster worker node, enforce the prepared AppArmor profile

~~~
#include<tunables/global>
profile docker-nginx flags=(attach_disconnected,mediate_deleted) {
#include<abstractions/base>
network inet tcp,
network inet udp,
network inet icmp,
deny network raw,
deny network packet,
file,
umount,
deny /bin/** wl,
deny /boot/** wl,
deny /dev/** wl,
deny /etc/** wl,
deny /home/** wl,
deny /lib/** wl,
deny /lib64/** wl,
deny /media/** wl,
deny /mnt/** wl,
deny /opt/** wl,
deny /proc/** wl,
deny /root/** wl,
deny /sbin/** wl,
deny /srv/** wl,
deny /tmp/** wl,
deny /sys/** wl,
deny /usr/** wl,
audit /** w,
/var/run/nginx.pid w,
/usr/sbin/nginx ix,
deny /bin/dash mrwklx,
deny /bin/sh mrwklx,
deny /usr/bin/top mrwklx,
capability chown,
capability dac_override,
capability setuid,
capability setgid,
capability net_bind_service,
deny @{PROC}/* w, # deny write for all files directly in /proc (not in a subdir)
# deny write to files not in /proc/<number>/** or /proc/sys/**
deny@{PROC}/{[^1-9],[^1-9][^0-9],[^1-9s][^0-9y][^0-9s],[^1-9][^0-9][^0-9][^0-9]*}/** w,
deny @{PROC}/sys/[^k]** w, # deny /proc/sys except /proc/sys/k* (effectively /proc/sys/kernel)
deny @{PROC}/sys/kernel/{?,??,[^s][^h][^m]**} w, # deny everything except shm* in /proc/sys/kernel/
deny @{PROC}/sysrq-trigger rwklx,
deny @{PROC}/mem rwklx,
deny @{PROC}/kmem rwklx,
deny @{PROC}/kcore rwklx,
deny mount,
deny /sys/[^f]*/** wklx,
deny /sys/f[^s]*/** wklx,
deny /sys/fs/[^c]*/** wklx,
deny /sys/fs/c[^g]*/** wklx,
deny /sys/fs/cg[^r]*/** wklx,
deny /sys/firmware/** rwklx,
deny /sys/kernel/security/** rwklx,
}
~~~

Edit the prepared manifest file to include the AppArmor profile.

~~~yaml
apiVersion: v1
kind: Pod
metadata:
		name:apparmor-pod
spec:
		containers:
		- name: apparmor-pod
			image: nginx
~~~

Finally, **apply the manifests files** and **create the Pod** specified on it.
Verify: Try to use command ping, top, sh





## 39 NetworkPolicy xxxxxx

A default-deny NetworkPolicy avoid to accidentally expose a Pod in a namespace that doesn't have any other NetworkPolicy defined.

**Create a new default-deny NetworkPolicy** named deny-network in the namespace test for all traffic of type Ingress + Egress
 The new NetworkPolicy must deny all Ingress + Egress traffic in the namespace test.
 **Apply the newly created default-deny NetworkPolicy** to all Pods running in namespace test.

~~~yaml
$ k get pods -n test --show-labels
uk.co.certification.simulator.questionpool.PList@e614300 $ vim netpol.yaml
uk.co.certification.simulator.questionpool.PList@e6144a0 master1 $ k apply -f netpol.yaml 

k get pods -n test --show-labels
NAME READY STATUS RESTARTS AGE LABELS test-pod 1/1 Running 0 34s role=test,run=test-pod testing 1/1 Running 0 17d run=testing master1

$ vim netpol1.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-network
namespace: test
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
~~~







## 40. Image Policy	 xxxxxx

A container image scanner is set up on the cluster.
 Given an incomplete configuration in the directory
 /etc/Kubernetes/confcontrol and a functional container image scanner 
with HTTPS endpoint https://acme.local.8081/image_policy

1. Enable the admission plugin.
2. Validate the control configuration and change it to implicit deny.

 Finally, test the configuration by deploying the pod having the image tag as the latest.



## 41. RuntimeClass - no xxxxxx

**Create a RuntimeClass** named gvisor-rc using the prepared runtime handler named **runsc**.
**Create a Pods** of image Nginx in the Namespace server to run on the gVisor runtime class

~~~sh
cat <<EOF | kubectl apply -f -
apiVersion: node.k8s.io/v1beta1
kind: RuntimeClass
metadata: name: gvisor
handler: runsc
EOF
~~~

~~~sh
cat <<EOF |kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
name: nginx-gvisor
spec:
runtimeClassName: gvisor
containers:
- name: nginx
  image: nginx
EOF
~~~

~~~
kubectl get pod nginx-gvisor -o wide
~~~







## 42. Ingress, Tls - no  xxxxxxxxx

**Create a Pod** name Nginx-pod inside the namespace testing, **Create a service** for the Nginx-pod named nginx-svc, using the ingress of your choice, **run the ingress on tls, secure port.**



## 43. NetworkPolicy - no xxxxxxx

**Create a network policy** named restrict-np to restrict to pod nginx-test running in namespace testing.
**Only allow the following Pods to connect to Pod** nginx-test:-

1. pods in the namespace default
2. pods with label version:v1 in any namespace.

Make sure to apply the network policy.



## 44. ServiceAccount - no xxxxxxx

Given an existing Pod named test-web-pod running in the namespace test-system
 Edit the existing Role bound to the Pod's **Service Account** named sa-backend to only allow performing get operations on endpoints.
 **Create** a new **Rolenamed** test-system-role-2 in the namespace test-system, which can perform patch operations, on resources of type statefulsets.
 **Create a new RoleBinding** named test-system-role-2-binding binding the newly created Role to the Pod's ServiceAccount sa-backend.













